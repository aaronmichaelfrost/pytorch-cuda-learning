{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4pbbhRs7G4X6oH/B0tjkN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--cl8VozDweD"
      },
      "outputs": [],
      "source": [
        "# Aaron Frost 2025\n",
        "\n",
        "# let's learn about how to profile CUDA kernels - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "#  I'm following along with https://www.youtube.com/watch?v=LuhJEEJQgUM&ab_channel=GPUMODE\n",
        "\n",
        "# in order to profile individual CUDA operations (kernels), we can't use the python time module.\n",
        "# this is because CUDA is ASYNC!\n",
        "# if you want to profile an operation you might use:\n",
        "#   start = torch.cuda.Event(enable_timing=True)  -- creates a start event\n",
        "#   end = torch.cuda.Event(enable_timing=True)    -- creates an end event\n",
        "\n",
        "# you also have to warm up CUDA before profiling\n",
        "#   the first time you call CUDA in a pytorch function it will initialize, so we want to get that out of the way first before starting a timer.\n",
        "\n",
        "# start.record() -- start the timer\n",
        "# // execute the function\n",
        "# end.record()   -- post the end event\n",
        "#\n",
        "# torch.cuda.synchronize()  -- AKA await the completion of the kernel\n",
        "\n",
        "# the time it took:\n",
        "# time = start.elapsed_time(end)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is torch autograd profiler?  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "\n",
        "# built in pytorch profiler tells you how much time each kernel took on CPU and GPU\n",
        "# gives you callstack, with the time it took at each method in the stack.\n",
        "\n",
        "# with torch.autograd.profiler.profile(use_cuda=True) as profiler:\n",
        "  # // do stuff that needs profiling\n",
        "\n",
        "# print out the table to see what the most time consuming kernels are\n",
        "# print(profiler.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ],
      "metadata": {
        "id": "Ci_vNUcnFGzl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is pytorch profiler?  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
        "#\n",
        "# visual profiler\n",
        "# doesn't give you debugging for kernel internals\n",
        "# JSON file you can drag and drop into google chrome.\n",
        "# you can see the CUDA kernels on teh pytorch github repo by looking for .cu"
      ],
      "metadata": {
        "id": "VLbwfGwBGAJD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to integrate custom CUDA kernel in PyTorch\n",
        "\n",
        "# basically load a C++ function in a python program\n",
        "#    easiest way is\n",
        "#       from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "\n",
        "# Ninja is required for this, so we need to execute this command in the runtime terminal:\n",
        "# apt-get install ninja-build\n",
        "\n",
        "# then you can write the .cu source inline:\n",
        "# ex.\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "cpp_source = \"\"\"\n",
        "std::string hello_world() {\n",
        "  return \"Hello World!\";\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# under the hood this codegens a makefile to run a compiler and produce CPP output files and binds them to python using PYBIND\n",
        "my_module = load_inline(\n",
        "    name='my_module',\n",
        "    cpp_sources=[cpp_source],\n",
        "    functions=['hello_world'],\n",
        "    verbose=True)\n",
        "\n",
        "print(my_module.hello_world())\n",
        "\n",
        "# https://www.youtube.com/watch?v=-6_CvTdzMRY&ab_channel=MarkSaroufim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xpNhZfpHBPt",
        "outputId": "4ad80d58-8570-4334-c679-42c376bff8bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module my_module, skipping build step...\n",
            "Loading extension module my_module...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a lot of machine learning progress comes down to a \"bag of tricks\" people in the ML community know about to make models converge faster\n",
        "# one trick is mixed-precision\n",
        "# all you have to do is make the weights and inputs half precision...\n",
        "\n",
        "# floating point data type has bits for mantissa and exponent (binary scientific notation)\n",
        "# model size is a proxy for training time required.\n",
        "\n",
        "# one more subtle thing you have to do:\n",
        "#\n",
        "# example: batch normalization - make sure outputs (activations) at any individual layer are not too big or too small:\n",
        "#   when you do this, using a lower precision would cause a problem:\n",
        "#     smaller floating point types (less bits) means it can't have as many decimal points... so we're losing information with each batch norm\n",
        "# introducing mixed precision: (loss-gradient scaling)\n",
        "#   maintain copy of single precision (32 bit) floats\n",
        "#       copy to half precision (16 bit) -> do foward propagation, multiply by scaling factor (to make it larger / get rid of decimal points),\n",
        "#            do backprop, multiply weight gradient by 1/scaling factor, to make it smaller again"
      ],
      "metadata": {
        "id": "UQBEMhTLXVOt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING CUDA:\n",
        "#\n",
        "# you can move a tensor to the GPU using .cuda()\n",
        "# GPU only works well for tasks that can be broken down into many smaller parallel tasks, like traning neural networks\n",
        "\n",
        "# common CUDA optimizations:\n",
        "#\n",
        "# Memory coalescing:\n",
        "#   https://youtu.be/mLxZyWOI340?si=A4Kbj-OZvrLY8Jf-\n",
        "#   GPU is most effienct when threads read or write contiguous global memory locations\n",
        "#       - coalesced access - as opposed to strided (stride between each access) - reduces the number of memory transactions required\n",
        "#   To programmers, a tensor might look like a square, but in RAM, it's a single linear set of addresses.\n",
        "#     It's a perf optimization to SHARE memory access.\n",
        "#     When each thread needs to access a different col of a matrix, it is more optimal than if each row needs to be accessed.\n",
        "#         --> for this reason you might transpose the rows and cols.\n",
        "\n",
        "\n",
        "# when you run a kernel, you define the block and how many threads are in the block, and grid layout (how many blocks)\n",
        "# each thread block is assigned to a streaming multiprocessor - each can process a number of threads\n",
        "\n",
        "\n",
        "# Shared memory:\n",
        "#     Shared memory is a fast, user-managed memory that is shared among all threads in the same thread block.\n",
        "#     Declare shared memory __shared__ float sharedArray[BLOCK_SIZE];\n",
        "\n",
        "\n",
        "#     Example.. Matrix multiplication\n",
        "#       Things to look out for: Bank conflicts degrading perf, limited memory size, ensuring to sync the threads.\n",
        "\"\"\"\n",
        "__global__ void matrixMul(float *A, float *B, float *C, int N) {\n",
        "    __shared__ float Asub[TILE_SIZE][TILE_SIZE];\n",
        "    __shared__ float Bsub[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int row = blockIdx.y * TILE_SIZE + ty;\n",
        "    int col = blockIdx.x * TILE_SIZE + tx;\n",
        "\n",
        "    float value = 0;\n",
        "\n",
        "    for (int i = 0; i < N / TILE_SIZE; ++i) {\n",
        "        // Load tiles into shared memory\n",
        "        Asub[ty][tx] = A[row * N + (i * TILE_SIZE + tx)];\n",
        "        Bsub[ty][tx] = B[(i * TILE_SIZE + ty) * N + col];\n",
        "        __syncthreads(); // sync to ensure data-consistency\n",
        "\n",
        "        // Perform multiplication\n",
        "        for (int j = 0; j < TILE_SIZE; ++j) {\n",
        "            value += Asub[ty][j] * Bsub[j][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result to global memory\n",
        "    C[row * N + col] = value;\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "Vugyllhxcpys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}